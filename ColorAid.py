# -*- coding: utf-8 -*-
"""Copy of abcd_detectron.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BTBRJE2oE_VU1G1JcLHVFCnG43c6HsEL
"""

import cv2
import numpy as np
import torch
import os
import json
import hashlib
import warnings
import pandas as pd
import matplotlib.pyplot as plt
from skimage.color import rgb2lab, lab2rgb, deltaE_ciede2000
import gradio as gr

from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg
from detectron2 import model_zoo
from detectron2.utils.visualizer import Visualizer
from detectron2.data import MetadataCatalog

# Dictionary of color blindness simulation matrices
COLOR_BLINDNESS_MATRICES = {
    "protanopia": np.array([[0.567, 0.433, 0.0], [0.558, 0.442, 0.0], [0.0, 0.242, 0.758]]),
    "deuteranopia": np.array([[0.625, 0.375, 0.0], [0.7, 0.3, 0.0], [0.0, 0.3, 0.7]]),
    "tritanopia": np.array([[0.95, 0.05, 0.0], [0.0, 0.433, 0.567], [0.0, 0.475, 0.525]]),
    "protanomaly": np.array([[0.817, 0.183, 0.0], [0.333, 0.667, 0.0], [0.0, 0.125, 0.875]]),
    "deuteranomaly": np.array([[0.8, 0.2, 0.0], [0.258, 0.742, 0.0], [0.0, 0.142, 0.858]]),
    "tritanomaly": np.array([[0.967, 0.033, 0.0], [0.0, 0.733, 0.267], [0.0, 0.183, 0.817]]),
    "achromatopsia": np.array([[0.299, 0.587, 0.114], [0.299, 0.587, 0.114], [0.299, 0.587, 0.114]]),
    "achromatomaly": np.array([[0.618, 0.320, 0.062], [0.163, 0.775, 0.062], [0.163, 0.320, 0.516]])
}

# Output directory
OUT_DIR = "/tmp/cvd_outputs"
os.makedirs(OUT_DIR, exist_ok=True)

# ------------------------ Utilities ------------------------

def simulate_color_blindness(image, deficiency_type):
    if deficiency_type not in COLOR_BLINDNESS_MATRICES:
        raise ValueError(f"Invalid deficiency type. Choose from {list(COLOR_BLINDNESS_MATRICES.keys())}.")

    if isinstance(image, str):
        img = cv2.imread(image)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    else:
        img = image.copy()
        if len(img.shape) == 2:
            img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)
        elif img.shape[2] == 4:
            img = cv2.cvtColor(img, cv2.COLOR_RGBA2RGB)

    normalized_image = img.astype(float) / 255.0
    transform_matrix = COLOR_BLINDNESS_MATRICES[deficiency_type]
    transformed_image = np.dot(normalized_image.reshape(-1, 3), transform_matrix.T)
    transformed_image = np.clip(transformed_image, 0, 1).reshape(img.shape)
    transformed_image = (transformed_image * 255).astype(np.uint8)
    return transformed_image


def image_sha1(rgb_image: np.ndarray) -> str:
    ok, buf = cv2.imencode(".png", cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR))
    if not ok:
        raise RuntimeError("Failed to encode image for hashing")
    return hashlib.sha1(buf.tobytes()).hexdigest()


def save_image(path: str, rgb: np.ndarray):
    cv2.imwrite(path, cv2.cvtColor(rgb, cv2.COLOR_RGB2BGR))


def gamut_safe(rgb_float: np.ndarray) -> np.ndarray:
    return (np.clip(rgb_float, 0.0, 1.0) * 255).astype(np.uint8)


def deltaE2000_mean(mask: np.ndarray, lab_a: np.ndarray, lab_b: np.ndarray) -> float:
    if mask is None:
        de = deltaE_ciede2000(lab_a, lab_b)
        return float(np.mean(de))
    idx = mask == 255
    if not np.any(idx):
        return 0.0
    de = deltaE_ciede2000(lab_a[idx], lab_b[idx])
    return float(np.mean(de))


def deltaE2000_heatmap(rgb_a: np.ndarray, rgb_b: np.ndarray, vmax: float = 30.0) -> np.ndarray:
    lab_a = rgb2lab(rgb_a.astype(np.float32) / 255.0)
    lab_b = rgb2lab(rgb_b.astype(np.float32) / 255.0)
    de = deltaE_ciede2000(lab_a, lab_b)
    de_norm = np.clip(de / vmax, 0, 1)
    heat = (de_norm * 255).astype(np.uint8)
    heat_color = cv2.applyColorMap(heat, cv2.COLORMAP_PLASMA)
    return cv2.cvtColor(heat_color, cv2.COLOR_BGR2RGB)

# ------------------------ Detectron2 ------------------------

MODEL_CONFIGS = {
    "R50-FPN-3x": "COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml",
    "R50-FPN-1x": "COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.yaml",
}


def setup_detectron2_model(model_name: str = "R50-FPN-3x", score_thresh: float = 0.5, min_size: int = 800):
    cfg = get_cfg()
    cfg.MODEL.DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    cfg.merge_from_file(model_zoo.get_config_file(MODEL_CONFIGS.get(model_name, MODEL_CONFIGS["R50-FPN-3x"])) )
    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(MODEL_CONFIGS.get(model_name, MODEL_CONFIGS["R50-FPN-3x"]))
    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = float(score_thresh)
    cfg.INPUT.MIN_SIZE_TEST = int(min_size)
    cfg.INPUT.MAX_SIZE_TEST = max(1280, int(min_size) * 2)
    return DefaultPredictor(cfg), cfg


def get_detection_cache_path(img_hash: str, model_name: str, score_thresh: float, min_size: int) -> str:
    key = f"{img_hash}_{model_name}_{score_thresh:.2f}_{min_size}"
    return os.path.join(OUT_DIR, f"det_{key}.json")


def detect_objects(image, predictor, cfg):
    if len(image.shape) == 3 and image.shape[2] == 3:
        image_bgr = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
    else:
        image_bgr = image

    if cfg.MODEL.DEVICE == "cuda":
        with torch.autocast(device_type="cuda", dtype=torch.float16):
            outputs = predictor(image_bgr)
    else:
        outputs = predictor(image_bgr)

    instances = outputs["instances"].to("cpu")
    boxes = instances.pred_boxes.tensor.numpy() if instances.has("pred_boxes") else np.zeros((0,4))
    scores = instances.scores.numpy() if instances.has("scores") else np.array([])
    class_ids = instances.pred_classes.numpy() if instances.has("pred_classes") else np.array([])

    metadata = MetadataCatalog.get(cfg.DATASETS.TRAIN[0])
    class_names = metadata.thing_classes

    v = Visualizer(image_bgr, metadata=metadata, scale=1.2)
    vis_output = v.draw_instance_predictions(instances)
    vis_image = vis_output.get_image()
    vis_image = cv2.cvtColor(vis_image, cv2.COLOR_BGR2RGB)

    detected_objects = []
    masks = instances.pred_masks.numpy() if instances.has("pred_masks") else None

    for i in range(len(boxes)):
        class_name = class_names[class_ids[i]] if int(class_ids[i]) < len(class_names) else "Unknown"
        bbox_data = {
            "class": class_name,
            "confidence": float(scores[i]) if len(scores) > i else 0.0,
            "bounding_box": {
                "x1": float(boxes[i][0]),
                "y1": float(boxes[i][1]),
                "x2": float(boxes[i][2]),
                "y2": float(boxes[i][3]),
            }
        }
        if masks is not None:
            mask = (masks[i] * 255).astype(np.uint8)
            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            bbox_data["edges"] = [contour.reshape(-1, 2).tolist() for contour in contours]
        detected_objects.append(bbox_data)

    return detected_objects, vis_image


def save_detected_objects_with_edges(detected_objects, output_path):
    edge_data = [obj for obj in detected_objects if "edges" in obj]
    with open(output_path, "w") as f:
        json.dump(edge_data, f, indent=2)
    return output_path

# ------------------------ Analysis & Enhancement ------------------------

def analyze_iou_and_lab_similarity(image_rgb, json_path, color_threshold=20.0, iou_threshold=0.0):
    height, width = image_rgb.shape[:2]

    with open(json_path, "r") as f:
        objects = json.load(f)

    def contour_to_mask(edges):
        mask = np.zeros((height, width), dtype=np.uint8)
        for edge in edges:
            contour = np.array(edge, dtype=np.int32).reshape((-1, 1, 2))
            cv2.drawContours(mask, [contour], -1, 255, thickness=cv2.FILLED)
        return mask

    def get_avg_lab(mask, image_rgb):
        masked_pixels = image_rgb[mask == 255]
        if masked_pixels.size == 0:
            return np.zeros(3, dtype=np.float32)
        lab = rgb2lab(masked_pixels.reshape(-1, 1, 3) / 255.0).reshape(-1, 3)
        return np.mean(lab, axis=0)

    def compute_iou(mask1, mask2):
        inter = np.logical_and(mask1 == 255, mask2 == 255).sum()
        union = np.logical_or(mask1 == 255, mask2 == 255).sum()
        return inter / union if union > 0 else 0.0

    masks = [contour_to_mask(obj["edges"]) for obj in objects]
    lab_means = [get_avg_lab(mask, image_rgb) for mask in masks]

    similar_pairs = []
    for i in range(len(objects)):
        for j in range(i + 1, len(objects)):
            iou = compute_iou(masks[i], masks[j])
            if iou >= iou_threshold:
                lab_i = lab_means[i].reshape(1, 1, 3)
                lab_j = lab_means[j].reshape(1, 1, 3)
                deltaE = float(deltaE_ciede2000(lab_i, lab_j))
                if deltaE < color_threshold:
                    similar_pairs.append({
                        "obj1_index": i,
                        "obj2_index": j,
                        "label1": objects[i]["class"],
                        "label2": objects[j]["class"],
                        "iou": float(iou),
                        "deltaE": float(deltaE)
                    })

    highlighted = image_rgb.copy()
    for pair in similar_pairs:
        for obj_id in [pair["obj1_index"], pair["obj2_index"]]:
            contours = [np.array(edge, np.int32).reshape((-1, 1, 2)) for edge in objects[obj_id]["edges"]]
            cv2.drawContours(highlighted, contours, -1, (0, 255, 0), thickness=3)

    return similar_pairs, highlighted, masks


def adjust_similar_object_colors(image_rgb, masks, similar_pairs, color_shift=60, shift_mode='auto'):
    lab_image = rgb2lab(image_rgb.astype(np.float32) / 255.0)
    adjusted_image = lab_image.copy()
    adjusted_ids = set()

    for pair in similar_pairs:
        target_id = pair["obj2_index"]
        if target_id in adjusted_ids:
            continue
        mask = masks[target_id]
        object_pixels = adjusted_image[mask == 255]
        if object_pixels.size == 0:
            continue
        mean_color = np.mean(object_pixels, axis=0)
        if shift_mode == 'a':
            shift = np.array([0, color_shift, 0])
        elif shift_mode == 'b':
            shift = np.array([0, 0, color_shift])
        else:
            shift = np.array([0, 0, 0])
            if abs(mean_color[1]) < abs(mean_color[2]):
                shift[1] = color_shift
            else:
                shift[2] = color_shift
        for i in range(3):
            channel = adjusted_image[:, :, i]
            low, high = (-128, 127) if i > 0 else (0, 100)
            channel[mask == 255] = np.clip(channel[mask == 255] + shift[i], low, high)
            adjusted_image[:, :, i] = channel
        adjusted_ids.add(target_id)

    adjusted_rgb = lab2rgb(adjusted_image)
    adjusted_rgb = gamut_safe(adjusted_rgb)

    border = np.zeros(image_rgb.shape[:2], dtype=np.uint8)
    for pair in similar_pairs:
        m = masks[pair["obj2_index"]]
        border = cv2.bitwise_or(border, m)
    if np.any(border):
        blur_mask = cv2.GaussianBlur(border, (0, 0), sigmaX=1.2, sigmaY=1.2)
        blur_mask_f = (blur_mask.astype(np.float32) / 255.0)[..., None]
        adjusted_rgb = (adjusted_rgb.astype(np.float32) * blur_mask_f + image_rgb.astype(np.float32) * (1 - blur_mask_f)).astype(np.uint8)

    return adjusted_rgb


def simulate_color_blindness_on_region(rgb_image, deficiency_type, region_mask):
    if deficiency_type not in COLOR_BLINDNESS_MATRICES:
        raise ValueError(f"Invalid deficiency type. Choose from {list(COLOR_BLINDNESS_MATRICES.keys())}.")
    transformed_image = rgb_image.copy().astype(np.float32) / 255.0
    mask_indices = np.where(region_mask == 255)
    pixels = transformed_image[mask_indices]
    transformed_pixels = np.dot(pixels, COLOR_BLINDNESS_MATRICES[deficiency_type].T)
    transformed_pixels = np.clip(transformed_pixels, 0, 1)
    transformed_image[mask_indices] = transformed_pixels
    transformed_image = (transformed_image * 255).astype(np.uint8)
    return transformed_image


def get_default_shift_mode(deficiency_type):
    if deficiency_type in ['protanopia', 'deuteranopia', 'protanomaly', 'deuteranomaly']:
        return 'b'
    elif deficiency_type in ['tritanopia', 'tritanomaly']:
        return 'a'
    else:
        return 'auto'

# ------------------------ Pipelines ------------------------

def perf_to_min_size(choice: str) -> int:
    return {"Speed": 640, "Balanced": 800, "Quality": 1024}.get(choice, 800)


def run_cvd_enhancement(image, deficiency_type, color_threshold, iou_threshold, color_shift, shift_mode, max_iterations,
                        model_name, score_thresh, performance_choice, show_heatmap=False, progress: gr.Progress = gr.Progress()):
    if image is None:
        return None, None, None, None, None, None, "Error: No image uploaded", None, None, None, None, None

    if isinstance(image, str):
        image = cv2.imread(image)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    elif len(image.shape) == 2:
        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)
    elif image.shape[2] == 4:
        image = cv2.cvtColor(image, cv2.COLOR_RGBA2RGB)

    original_path = os.path.join(OUT_DIR, "original.jpg")
    save_image(original_path, image)

    progress(0.05, desc="Simulating CVD")
    transformed_image = simulate_color_blindness(image, deficiency_type)
    cvd_path = os.path.join(OUT_DIR, "cvd.jpg")
    save_image(cvd_path, transformed_image)

    progress(0.15, desc="Setting up detector")
    img_hash = image_sha1(image)
    min_size = perf_to_min_size(str(performance_choice))
    cache_json = get_detection_cache_path(img_hash, model_name, float(score_thresh), int(min_size))
    if os.path.exists(cache_json):
        with open(cache_json, "r") as f:
            detected_objects = json.load(f)
        predictor, cfg = setup_detectron2_model(model_name, score_thresh, min_size)
        vis_image = image.copy()
        try:
            meta = MetadataCatalog.get(cfg.DATASETS.TRAIN[0])
            v = Visualizer(cv2.cvtColor(image, cv2.COLOR_RGB2BGR), metadata=meta, scale=1.2)
            for obj in detected_objects:
                if "edges" in obj:
                    for edge in obj["edges"]:
                        contour = np.array(edge, dtype=np.int32).reshape((-1, 1, 2))
                        cv2.drawContours(v.output.get_image(), [contour], -1, (0, 255, 0), 2)
            vis_image = cv2.cvtColor(v.output.get_image(), cv2.COLOR_BGR2RGB)
        except Exception:
            vis_image = image.copy()
    else:
        predictor, cfg = setup_detectron2_model(model_name, score_thresh, min_size)
        progress(0.25, desc="Running detection")
        detected_objects, vis_image = detect_objects(image, predictor, cfg)
        save_detected_objects_with_edges(detected_objects, cache_json)

    detection_path = os.path.join(OUT_DIR, "detection.jpg")
    save_image(detection_path, vis_image)

    progress(0.35, desc="Analyzing objects")
    similar_pairs, highlighted, masks = analyze_iou_and_lab_similarity(
        transformed_image,
        cache_json,
        color_threshold=float(color_threshold),
        iou_threshold=float(iou_threshold)
    )
    similar_path = os.path.join(OUT_DIR, "similar.jpg")
    save_image(similar_path, highlighted)

    if not similar_pairs:
        heatmap_img = deltaE2000_heatmap(transformed_image, transformed_image) if show_heatmap else None
        if show_heatmap:
            save_image(os.path.join(OUT_DIR, "heatmap.jpg"), heatmap_img)
        return image, transformed_image, vis_image, highlighted, transformed_image, heatmap_img, \
               "No similar object pairs found. No enhancement needed.", \
               original_path, cvd_path, os.path.join(OUT_DIR, "enhanced.jpg"), cache_json, None

    iteration = 0
    adjusted_image = transformed_image.copy()
    log_messages = [f"Found {len(similar_pairs)} similar object pairs. Starting enhancement..."]

    while iteration < int(max_iterations) and similar_pairs:
        progress(0.45 + 0.4 * (iteration / max(1, int(max_iterations))), desc=f"Enhancing colors (iter {iteration+1})")
        adjusted_rgb = adjust_similar_object_colors(
            adjusted_image,
            masks,
            similar_pairs,
            color_shift=float(color_shift),
            shift_mode=shift_mode
        )
        adjusted_mask = np.zeros(image.shape[:2], dtype=np.uint8)
        for pair in similar_pairs:
            adjusted_mask = cv2.bitwise_or(adjusted_mask, masks[pair["obj2_index"]])
        adjusted_image = simulate_color_blindness_on_region(adjusted_rgb, deficiency_type, adjusted_mask)

        similar_pairs, highlighted, masks = analyze_iou_and_lab_similarity(
            adjusted_image,
            cache_json,
            color_threshold=float(color_threshold),
            iou_threshold=float(iou_threshold)
        )
        log_messages.append(f"Iteration {iteration + 1}: {len(similar_pairs)} similar pairs remaining")
        if len(similar_pairs) == 0:
            log_messages.append("All similar objects now distinguishable. Enhancement complete!")
            break
        iteration += 1

    enhanced_path = os.path.join(OUT_DIR, "enhanced.jpg")
    save_image(enhanced_path, adjusted_image)

    heatmap_img = None
    if show_heatmap:
        progress(0.9, desc="Computing ΔE heatmap")
        heatmap_img = deltaE2000_heatmap(transformed_image, adjusted_image)
        save_image(os.path.join(OUT_DIR, "heatmap.jpg"), heatmap_img)

    progress(1.0, desc="Done")

    return image, transformed_image, vis_image, highlighted, adjusted_image, heatmap_img, \
           "\n".join(log_messages), \
           original_path, cvd_path, enhanced_path, cache_json, None


# =============== Auto-Tuning ===============

def auto_tune_and_enhance(image, deficiency_type, tuning_trials,
                          model_name, score_thresh, performance_choice, show_heatmap=False, progress: gr.Progress = gr.Progress()):
    if image is None:
        return None, None, None, None, None, None, "Error: No image uploaded", None, None, None, None, None

    if isinstance(image, str):
        base_img = cv2.imread(image)
        base_img = cv2.cvtColor(base_img, cv2.COLOR_BGR2RGB)
    elif len(image.shape) == 2:
        base_img = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)
    elif image.shape[2] == 4:
        base_img = cv2.cvtColor(image, cv2.COLOR_RGBA2RGB)
    else:
        base_img = image.copy()

    save_image(os.path.join(OUT_DIR, "original.jpg"), base_img)
    transformed_image = simulate_color_blindness(base_img, deficiency_type)
    save_image(os.path.join(OUT_DIR, "cvd.jpg"), transformed_image)

    img_hash = image_sha1(base_img)
    min_size = perf_to_min_size(str(performance_choice))
    cache_json = get_detection_cache_path(img_hash, model_name, float(score_thresh), int(min_size))
    predictor, cfg = setup_detectron2_model(model_name, score_thresh, min_size)
    if os.path.exists(cache_json):
        with open(cache_json, "r") as f:
            detected_objects = json.load(f)
        vis_image = base_img.copy()
        try:
            meta = MetadataCatalog.get(cfg.DATASETS.TRAIN[0])
            v = Visualizer(cv2.cvtColor(base_img, cv2.COLOR_RGB2BGR), metadata=meta, scale=1.2)
            for obj in detected_objects:
                if "edges" in obj:
                    for edge in obj["edges"]:
                        contour = np.array(edge, dtype=np.int32).reshape((-1, 1, 2))
                        cv2.drawContours(v.output.get_image(), [contour], -1, (0, 255, 0), 2)
            vis_image = cv2.cvtColor(v.output.get_image(), cv2.COLOR_BGR2RGB)
        except Exception:
            pass
    else:
        progress(0.05, desc="Running detection")
        detected_objects, vis_image = detect_objects(base_img, predictor, cfg)
        save_detected_objects_with_edges(detected_objects, cache_json)

    save_image(os.path.join(OUT_DIR, "detection.jpg"), vis_image)

    default_mode = get_default_shift_mode(deficiency_type)
    color_threshold_candidates = [15, 20, 25]
    iou_threshold_candidates = [0.0, 0.05, 0.1]
    color_shift_candidates = [30, 50, 70, 90]
    shift_mode_candidates = [default_mode] + [m for m in ['a', 'b'] if m != default_mode]
    max_iterations_candidates = [2, 3]

    rng = np.random.default_rng(0)
    all_candidates = [(ct, it, cs, sm, mi)
                      for ct in color_threshold_candidates
                      for it in iou_threshold_candidates
                      for cs in color_shift_candidates
                      for sm in shift_mode_candidates
                      for mi in max_iterations_candidates]

    n_trials = int(tuning_trials)
    if n_trials < len(all_candidates):
        idxs = rng.choice(len(all_candidates), size=n_trials, replace=False)
        candidates = [all_candidates[i] for i in idxs]
    else:
        candidates = all_candidates[:n_trials]

    best = {
        "score": (1e9, 1e9, -1),
        "params": None,
        "enhanced": None,
        "similar_highlight": None,
    }

    log_lines = [f"Auto-tuning {len(candidates)} trial(s) for {deficiency_type}..."]

    for trial_idx, (ct, it, cs, sm, mi) in enumerate(candidates, start=1):
        progress(0.1 + 0.7 * (trial_idx - 1) / max(1, len(candidates)), desc=f"Trial {trial_idx}/{len(candidates)}")

        pairs, _, masks = analyze_iou_and_lab_similarity(
            transformed_image, cache_json, color_threshold=float(ct), iou_threshold=float(it)
        )
        adjusted_image = transformed_image.copy()
        if pairs:
            iteration = 0
            while iteration < int(mi) and pairs:
                adjusted_rgb = adjust_similar_object_colors(
                    adjusted_image, masks, pairs, color_shift=float(cs), shift_mode=sm
                )
                adjusted_mask = np.zeros(base_img.shape[:2], dtype=np.uint8)
                for p in pairs:
                    adjusted_mask = cv2.bitwise_or(adjusted_mask, masks[p["obj2_index"]])
                adjusted_image = simulate_color_blindness_on_region(adjusted_rgb, deficiency_type, adjusted_mask)
                pairs, _, masks = analyze_iou_and_lab_similarity(
                    adjusted_image, cache_json, color_threshold=float(ct), iou_threshold=float(it)
                )
                iteration += 1

        final_pairs, highlighted, _ = analyze_iou_and_lab_similarity(
            adjusted_image, cache_json, color_threshold=float(ct), iou_threshold=float(it)
        )
        remaining = len(final_pairs)
        score = (remaining, cs, ct)
        improved = False
        if score < best["score"]:
            improved = True
            best.update({
                "score": score,
                "params": {"color_threshold": ct, "iou_threshold": it, "color_shift": cs, "shift_mode": sm, "max_iterations": mi},
                "enhanced": adjusted_image,
                "similar_highlight": highlighted,
            })
        log_lines.append(
            f"Trial {trial_idx}: ct={ct}, iou={it}, shift={cs}, mode={sm}, iters={mi} -> remaining={remaining}" + (" [best]" if improved else "")
        )
        if remaining == 0:
            log_lines.append("Early stop: 0 similar pairs remaining.")
            break

    if best["params"] is None:
        return base_img, transformed_image, vis_image, transformed_image, transformed_image, None, "Auto-tuning failed to evaluate.", \
               os.path.join(OUT_DIR, "original.jpg"), os.path.join(OUT_DIR, "cvd.jpg"), os.path.join(OUT_DIR, "enhanced.jpg"), cache_json, None

    best_params = best["params"]
    params_str = ", ".join([f"{k}={v}" for k, v in best_params.items()])
    log_lines.append(f"Best: {params_str} with remaining={best['score'][0]}")

    enhanced_path = os.path.join(OUT_DIR, "enhanced.jpg")
    save_image(enhanced_path, best["enhanced"])

    heatmap_img = None
    if show_heatmap:
        heatmap_img = deltaE2000_heatmap(transformed_image, best["enhanced"])
        save_image(os.path.join(OUT_DIR, "heatmap.jpg"), heatmap_img)

    return (
        base_img,
        transformed_image,
        vis_image,
        best["similar_highlight"],
        best["enhanced"],
        heatmap_img,
        "\n".join(log_lines),
        os.path.join(OUT_DIR, "original.jpg"),
        os.path.join(OUT_DIR, "cvd.jpg"),
        enhanced_path,
        cache_json,
        best_params,
    )

# ------------------------ Interface ------------------------

def create_interface():
    custom_css = """
    .app-header { text-align: center; margin: 0 auto; max-width: 1200px; padding: 1rem 0; }
    .app-title { font-size: 2.5rem !important; font-weight: bold; margin-bottom: 0.5rem; color: #1f77b4; }
    .app-description { font-size: 1.2rem !important; margin-bottom: 2rem; max-width: 900px; margin-left: auto; margin-right: auto; }
    .how-it-works { background-color: #222; color: #fff; border-radius: 8px; padding: 1.5rem; margin-bottom: 2rem; font-size: 1.2rem !important; max-width: 1000px; margin-left: auto; margin-right: auto; box-shadow: 0 2px 10px rgba(0,0,0,0.2);}
    .how-it-works h3 { text-align: center; font-size: 1.5rem !important; margin-bottom: 1rem; color: #fff;}
    .upload-image-container .wrap { display: flex !important; justify-content: center !important; align-items: center !important; height: auto !important; min-height: 450px !important; }
    .upload-image-container img, .result-image-container img { max-width: 100% !important; max-height: 100% !important; width: auto !important; height: auto !important; object-fit: contain !important; }
    .result-image-container { height: auto !important; min-height: 450px !important; display: flex !important; align-items: center !important; justify-content: center !important; }
    .results-header { text-align: center; font-size: 2rem !important; margin: 1.5rem 0; }
    .result-column h3 { text-align: center; font-size: 1.5rem !important; margin-bottom: 0.5rem; }
    .footer-button { display: block; margin: 2rem auto; max-width: 320px; }
    .detail-tabs .tab-nav { background-color: #f5f5f5; border-radius: 6px; padding: 5px; }
    .detail-tabs .tabitem { padding: 0.5rem !important; }
    """

    with gr.Blocks(title="Color Vision Deficiency Enhancement Tool", css=custom_css) as app:
        with gr.Row(elem_classes=["app-header"]):
            with gr.Column():
                gr.Markdown("# Color Vision Deficiency Enhancement Tool", elem_classes=["app-title"])
                gr.Markdown(
                    "Transform images for better visibility by people with color vision deficiency. "
                    "This intelligent system detects objects in images, identifies those that might appear similar to people with color blindness, "
                    "and enhances color differences to make them more distinguishable.",
                    elem_classes=["app-description"]
                )

        with gr.Row(visible=True, elem_classes=["how-it-works-container"]) as how_it_works_section:
            with gr.Column():
                gr.Markdown(
                    """
                    <div class="how-it-works">
                    <h3>How It Works</h3>
                    <ol>
                    <li><strong>Upload</strong> your image that needs enhancement</li>
                    <li><strong>Select</strong> the type of color vision deficiency to accommodate</li>
                    <li>The system <strong>detects objects</strong> in your image using AI</li>
                    <li>It <strong>finds objects</strong> that might look similar to someone with the selected CVD</li>
                    <li>Colors are <strong>enhanced</strong> to make objects more distinguishable</li>
                    <li>View side-by-side <strong>comparisons</strong> of original, CVD simulation, heatmap, and enhanced versions</li>
                    </ol>
                    <p>Fine-tune the enhancement with advanced settings for optimal results.</p>
                    </div>
                    """
                )

        with gr.Row(visible=True) as input_section:
            with gr.Column(scale=3):
                input_image = gr.Image(label="Upload Image", type="numpy", elem_classes=["upload-image-container"], height=450, image_mode="RGB", sources=["upload", "clipboard"])
            with gr.Column(scale=2):
                deficiency_type = gr.Dropdown(choices=list(COLOR_BLINDNESS_MATRICES.keys()), value="protanopia", label="Color Vision Deficiency Type")
                with gr.Accordion("Advanced Settings", open=False):
                    color_threshold = gr.Slider(minimum=5, maximum=50, value=20, step=1, label="Color Similarity Threshold (ΔE)")
                    iou_threshold = gr.Slider(minimum=0, maximum=0.5, value=0, step=0.01, label="Object Overlap Threshold (IoU)")
                    color_shift = gr.Slider(minimum=20, maximum=100, value=50, step=5, label="Color Shift Amount")
                    shift_mode = gr.Radio(["auto", "a", "b"], value="b", label="Color Shift Mode")
                    max_iterations = gr.Slider(minimum=1, maximum=10, value=3, step=1, label="Maximum Enhancement Iterations")
                    tuning_trials = gr.Slider(minimum=1, maximum=20, value=8, step=1, label="Auto-Tuning Trials (Colab-safe)")
                    model_name = gr.Dropdown(choices=list(MODEL_CONFIGS.keys()), value="R50-FPN-3x", label="Detectron2 Model")
                    score_thresh = gr.Slider(minimum=0.1, maximum=0.9, value=0.5, step=0.05, label="Detection Score Threshold")
                    performance = gr.Radio(["Speed", "Balanced", "Quality"], value="Balanced", label="Performance Preset")
                    show_heatmap = gr.Checkbox(value=False, label="Show ΔE Heatmap")
                deficiency_type.change(fn=lambda x: gr.update(value=get_default_shift_mode(x)), inputs=[deficiency_type], outputs=[shift_mode])
                run_button = gr.Button("Run Enhancement", variant="primary", size="lg")
                tune_button = gr.Button("Auto Tune & Enhance", variant="primary", size="lg")
                apply_best_button = gr.Button("Apply Best Params", variant="secondary")

        with gr.Row(visible=False) as processing_section:
            output_log = gr.Textbox(label="Processing Status", lines=5)

        with gr.Row(visible=False, elem_classes=["results-header"]) as results_header:
            gr.Markdown("## Results")

        with gr.Row(visible=False, equal_height=True) as main_results:
            with gr.Column(elem_classes=["result-column"]):
                gr.Markdown("### Original")
                original_output = gr.Image(elem_classes=["result-image-container"])
                download_original = gr.DownloadButton("Download Original")
            with gr.Column(elem_classes=["result-column"]):
                gr.Markdown("### CVD Simulation")
                cvd_output = gr.Image(elem_classes=["result-image-container"])
                download_cvd = gr.DownloadButton("Download CVD")
            with gr.Column(elem_classes=["result-column"]):
                gr.Markdown("### Enhanced for CVD")
                enhanced_output = gr.Image(elem_classes=["result-image-container"])
                download_enhanced = gr.DownloadButton("Download Enhanced")

        with gr.Row(visible=False) as detailed_results:
            with gr.Column():
                with gr.Tabs(elem_classes=["detail-tabs"]) as tabs:
                    with gr.TabItem("Object Detection", id="tab-detection"):
                        detection_output = gr.Image(elem_classes=["result-image-container"], height=350)
                        download_detection = gr.DownloadButton("Download Detection JSON")
                    with gr.TabItem("Similar Objects", id="tab-similar"):
                        similar_output = gr.Image(elem_classes=["result-image-container"], height=350)
                    with gr.TabItem("ΔE Heatmap", id="tab-heatmap"):
                        heatmap_output = gr.Image(elem_classes=["result-image-container"], height=350)

        with gr.Row(visible=False) as rerun_section:
            rerun_button = gr.Button("Process Another Image", variant="secondary", size="lg", elem_classes=["footer-button"])

        best_params_state = gr.State()

        def show_processing():
            return {input_section: gr.update(visible=False), how_it_works_section: gr.update(visible=False), processing_section: gr.update(visible=True), results_header: gr.update(visible=False), main_results: gr.update(visible=False), detailed_results: gr.update(visible=False), rerun_section: gr.update(visible=False)}
        def show_results():
            return {input_section: gr.update(visible=False), how_it_works_section: gr.update(visible=False), processing_section: gr.update(visible=False), results_header: gr.update(visible=True), main_results: gr.update(visible=True), detailed_results: gr.update(visible=True), rerun_section: gr.update(visible=True)}
        def reset_interface():
            return {input_section: gr.update(visible=True), how_it_works_section: gr.update(visible=True), processing_section: gr.update(visible=False), results_header: gr.update(visible=False), main_results: gr.update(visible=False), detailed_results: gr.update(visible=False), rerun_section: gr.update(visible=False)}

        def apply_best(params):
            if not params:
                return gr.update(), gr.update(), gr.update(), gr.update(), gr.update()
            return (
                gr.update(value=params.get("color_threshold", 20)),
                gr.update(value=params.get("iou_threshold", 0.0)),
                gr.update(value=params.get("color_shift", 50)),
                gr.update(value=params.get("shift_mode", "b")),
                gr.update(value=params.get("max_iterations", 3)),
            )

        def map_downloads(orig_path, cvd_path, enhanced_path, json_path):
            return orig_path, cvd_path, enhanced_path, json_path

        run_button.click(
            fn=show_processing,
            outputs=[input_section, how_it_works_section, processing_section, results_header, main_results, detailed_results, rerun_section]
        ).then(
            fn=run_cvd_enhancement,
            inputs=[input_image, deficiency_type, color_threshold, iou_threshold, color_shift, shift_mode, max_iterations,
                    model_name, score_thresh, performance, show_heatmap],
            outputs=[original_output, cvd_output, detection_output, similar_output, enhanced_output, heatmap_output, output_log,
                     download_original, download_cvd, download_enhanced, download_detection, best_params_state]
        ).then(
            fn=show_results,
            outputs=[input_section, how_it_works_section, processing_section, results_header, main_results, detailed_results, rerun_section]
        )

        tune_button.click(
            fn=show_processing,
            outputs=[input_section, how_it_works_section, processing_section, results_header, main_results, detailed_results, rerun_section]
        ).then(
            fn=auto_tune_and_enhance,
            inputs=[input_image, deficiency_type, tuning_trials, model_name, score_thresh, performance, show_heatmap],
            outputs=[original_output, cvd_output, detection_output, similar_output, enhanced_output, heatmap_output, output_log,
                     download_original, download_cvd, download_enhanced, download_detection, best_params_state]
        ).then(
            fn=show_results,
            outputs=[input_section, how_it_works_section, processing_section, results_header, main_results, detailed_results, rerun_section]
        )

        apply_best_button.click(
            fn=apply_best,
            inputs=[best_params_state],
            outputs=[color_threshold, iou_threshold, color_shift, shift_mode, max_iterations]
        )

        rerun_button.click(
            fn=reset_interface,
            outputs=[input_section, how_it_works_section, processing_section, results_header, main_results, detailed_results, rerun_section]
        )

    return app

# Load and run the interface
demo = create_interface()
demo.queue()
demo.launch(debug=True, share=True)

# Install required packages (Colab)
!pip install torch torchvision torchaudio
!pip install gradio scikit-image
!pip install 'git+https://github.com/facebookresearch/detectron2.git'